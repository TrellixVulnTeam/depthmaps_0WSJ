{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Adversarial Networks for Depth Maps Generation\n",
    "Following the 2017 paper by Pan et al: *Supervised Adversarial Networks for Image Saliency Detection*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.misc import imresize\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make3D data\n",
    "from Make3D import train_pairs, test_pairs\n",
    "\n",
    "# NYU data\n",
    "# from NYU import nyu_data\n",
    "# train_pairs, test_pairs = nyu_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (256, 256)\n",
    "\n",
    "train_data, train_targets = zip(*train_pairs)\n",
    "test_data, test_targets = zip(*test_pairs)\n",
    "\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[..., :3], [0.299, 0.587, 0.114])\n",
    "\n",
    "\n",
    "def resize(img):\n",
    "    return imresize(img, shape)\n",
    "\n",
    "\n",
    "train_data = [resize(rgb2gray(img)) for img in train_data]\n",
    "train_targets = [resize(img) for img in train_targets]\n",
    "test_data = [resize(rgb2gray(img)) for img in test_data]\n",
    "test_targets = [resize(img) for img in test_targets]\n",
    "\n",
    "train_x, train_t = np.asarray(train_data), np.asarray(train_targets)\n",
    "test_x, test_t = np.asarray(test_data), np.asarray(test_targets)\n",
    "\n",
    "print('train input/target shapes', train_data[0].shape, train_targets[0].shape)\n",
    "print('train input min/max/ptp', np.min(train_data),\n",
    "      np.max(train_data), np.ptp(train_data))\n",
    "print('train target min/max/ptp', np.min(train_targets),\n",
    "      np.max(train_targets), np.ptp(train_targets))\n",
    "\n",
    "tuples = zip(train_x[:10], train_t[:10])\n",
    "fig, axis = plt.subplots(5, 2, figsize=(10, 20))\n",
    "plt.tight_layout(), plt.gray()\n",
    "for (rgb, d), (ax1, ax2) in zip(tuples, axis):\n",
    "    ax1.axis('off'), ax2.axis('off')\n",
    "    ax1.imshow(rgb)\n",
    "    ax2.imshow(imresize(d, rgb.shape))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.framework import get_variables\n",
    "\n",
    "\n",
    "def lrelu(x, alpha=0.2):\n",
    "    \"\"\"Leaky rectifier.\"\"\"\n",
    "    return tf.maximum(alpha * x, x)\n",
    "\n",
    "\n",
    "def conv2d(inputs, num_outputs, kernel_size=(4, 4), strides=2,\n",
    "           padding='SAME', activation=lrelu, norm=False, training=False):\n",
    "    \"\"\"Wrapper for tf.layers.conv2d with default parameters.\"\"\"\n",
    "    net = tf.layers.conv2d(inputs, num_outputs, kernel_size, strides,\n",
    "                           padding=padding)\n",
    "    if norm:\n",
    "        net = tf.layers.batch_normalization(net, training=training)\n",
    "    return activation(net)\n",
    "\n",
    "\n",
    "def conv2d_transpose(inputs, num_outputs, kernel_size=(4, 4), strides=2,\n",
    "                     padding='SAME', activation=lrelu, norm=False,\n",
    "                     training=False):\n",
    "    \"\"\"Wrapper for tf.layers.conv2d_transpose with default parameters.\"\"\"\n",
    "    net = tf.layers.conv2d_transpose(inputs, num_outputs, kernel_size,\n",
    "                                     strides, padding=padding)\n",
    "    if norm:\n",
    "        net = tf.layers.batch_normalization(net, training=training)\n",
    "    return activation(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator(images, is_training):\n",
    "    \"\"\"Discriminator.\n",
    "\n",
    "    Args:\n",
    "        images (tf.Tensor[tf.float32]): Input images, either from generator\n",
    "            or real inputs. Shape (BATCH, 256, 256, (input_c + output_c))\n",
    "        is_training (tf.Tensor[tf.bool]): True when in training, False \n",
    "            when in testing step.\n",
    "\n",
    "    Returns:\n",
    "        (tf.Tensor): Sigmoid network output, single scalar in [0, 1].\n",
    "        (tf.Tensor): Linear network output, single scalar.\n",
    "        (List[tf.Operation]): Batch normalization update operations.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('discriminator') as scope:\n",
    "        net = conv2d(images, 64)\n",
    "        net = conv2d(net, 128, norm=True, training=is_training)\n",
    "        net = conv2d(net, 256, norm=True, training=is_training)\n",
    "        net = conv2d(net, 512, norm=True, training=is_training)\n",
    "        net = conv2d(net, 1, (1, 1))\n",
    "        logits = tf.layers.dense(net, 1, tf.identity)\n",
    "        out = tf.nn.sigmoid(logits)\n",
    "        ops = get_variables(scope, collection=tf.GraphKeys.UPDATE_OPS)\n",
    "        return out, logits, ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator(images, is_training):\n",
    "    \"\"\"Discriminator.\n",
    "\n",
    "    Args:\n",
    "        images (tf.Tensor[tf.float32]): Input images, either from generator\n",
    "            or real inputs. Shape (BATCH, 256, 256, (input_c + output_c))\n",
    "        is_training (tf.Tensor[tf.bool]): True when in training, False \n",
    "            when in testing step.\n",
    "\n",
    "    Returns:\n",
    "        (tf.Tensor): Tanh network output, single channel shaped as input.\n",
    "        (List[tf.Operation]): Batch normalization update operations.\n",
    "\n",
    "    TODO: Implement skipping between encoder and decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope('generator') as scope:\n",
    "        with tf.variable_scope('encoder'):\n",
    "            enc = conv2d(images, 64)\n",
    "            enc = conv2d(enc, 128, norm=True, training=is_training)\n",
    "            enc = conv2d(enc, 256, norm=True, training=is_training)\n",
    "            enc = conv2d(enc, 512, norm=True, training=is_training)\n",
    "            enc = conv2d(enc, 512, norm=True, training=is_training)\n",
    "            enc = conv2d(enc, 512, norm=True, training=is_training)\n",
    "            enc = conv2d(enc, 512, norm=True, training=is_training)\n",
    "            enc = conv2d(enc, 512, norm=True, training=is_training)\n",
    "        with tf.variable_scope('decoder'):\n",
    "            dec = enc\n",
    "            dec = conv2d_transpose(dec, 512, norm=True, training=is_training)\n",
    "            dec = tf.layers.dropout(dec, .5)\n",
    "            dec = conv2d_transpose(dec, 512, norm=True, training=is_training)\n",
    "            dec = tf.layers.dropout(dec, .5)\n",
    "            dec = conv2d_transpose(dec, 512, norm=True, training=is_training)\n",
    "            dec = tf.layers.dropout(dec, .5)\n",
    "            dec = conv2d_transpose(dec, 512, norm=True, training=is_training)\n",
    "            dec = conv2d_transpose(dec, 512, norm=True, training=is_training)\n",
    "            dec = conv2d_transpose(dec, 256, norm=True, training=is_training)\n",
    "            dec = conv2d_transpose(dec, 128, norm=True, training=is_training)\n",
    "            dec = conv2d_transpose(dec, 64, norm=True, training=is_training)\n",
    "            out = conv2d(dec, 1, (1, 1), 1, activation=tf.nn.tanh)\n",
    "        ops = get_variables(scope, collection=tf.GraphKeys.UPDATE_OPS)\n",
    "        return out, ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "is_training = tf.placeholder_with_default(False, None)\n",
    "images = tf.placeholder(tf.uint8, (None, 256, 256), name='inputs')\n",
    "depthmaps = tf.placeholder(tf.float32, (None, 256, 256), name='targets')\n",
    "\n",
    "\n",
    "def scale(img):\n",
    "    return ((tf.cast(img, tf.float32) / 255) - .5) * 2\n",
    "\n",
    "\n",
    "images_ = tf.reshape(scale(images), (-1, 256, 256, 1))\n",
    "depthmaps_ = tf.reshape(scale(depthmaps), (-1, 256, 256, 1))\n",
    "\n",
    "# Create generator (and sampler?)\n",
    "# remake_generator = tf.make_template('generator', make_generator)\n",
    "generator, g_ops = make_generator(images_, is_training)\n",
    "# sampler = remake_generator(images_, is_training)\n",
    "\n",
    "\n",
    "def soft_labels_like(like, real=False):\n",
    "    if real:\n",
    "        return tf.random_uniform(tf.shape(like), 0.7, 1.2, tf.float32)\n",
    "    else:\n",
    "        return tf.random_uniform(tf.shape(like), 0., 0.3, tf.float32)\n",
    "\n",
    "\n",
    "# Create the two discriminator graphs.\n",
    "real = tf.concat([images_, depthmaps_], axis=-1)\n",
    "fake = tf.concat([images_, generator], axis=-1)\n",
    "remake_discriminator = tf.make_template('discriminator', make_discriminator)\n",
    "d_real, d_logits_real, d_ops = remake_discriminator(real, is_training)\n",
    "d_fake, d_logits_fake, _ = remake_discriminator(fake, is_training)\n",
    "\n",
    "# Create discriminator losses.\n",
    "d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    logits=d_logits_real, labels=soft_labels_like(d_fake, real=True)))\n",
    "d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    logits=d_logits_fake, labels=soft_labels_like(d_fake, real=False)))\n",
    "d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "LAMBDA = 0\n",
    "\n",
    "# Create generator loss.\n",
    "g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    logits=d_logits_fake, labels=soft_labels_like(d_fake, real=True))) \\\n",
    "    + LAMBDA * tf.reduce_mean(tf.abs(depthmaps - generator))\n",
    "\n",
    "with tf.control_dependencies(g_ops):\n",
    "    g_op = tf.train.AdamOptimizer().minimize(g_loss)\n",
    "\n",
    "with tf.control_dependencies(d_ops):\n",
    "    d_op = tf.train.AdamOptimizer().minimize(d_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(x, y, batchsize=32):\n",
    "    permute = np.random.permutation(len(train_t))\n",
    "    for i in range(0, len(train_t) - 1, batchsize):\n",
    "        indices = permute[i * batchsize:i * batchsize + batchsize]\n",
    "        yield x[indices], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch = -1\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    for batch_x, batch_t in batches(train_x, train_t, 32):\n",
    "        batch += 1\n",
    "        if batch % 2 == 0:\n",
    "            sess.run(d_op, \n",
    "                     {images: batch_x, depthmaps: batch_t, is_training: True})\n",
    "        else:\n",
    "            sess.run(g_op, \n",
    "                     {images: batch_x, is_training: True})\n",
    "        print(batch)\n",
    "\n",
    "    dloss, gloss = sess.run([d_loss, g_loss],\n",
    "                            {images: test_x, depthmaps: test_t})\n",
    "\n",
    "    print('{}: {:.2f}, {:.2f}'.format(i, dloss, gloss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "depthmaps",
   "language": "python",
   "name": "depthmaps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
