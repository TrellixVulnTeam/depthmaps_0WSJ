# Results  {#sec:results}

## Simple

**TODO**: Results to be added.

## MultiScale

Our simplified implementation of @Eigen2014 produces reasonable, but not good results.

**TODO**: Results to be added.


## Pix2Pix

In figure @fig:gan_losses one can very well see the correlation between the discriminator and the generator loss: At 8am september 4th one can see that the discriminator got better at detecting images generated by the generator which resulted in the generator loss to grow a little.

![Generator and discriminator losses from the Pix2Pix model as exponential moving averages over two days of training. Regarding the overall goal of generating realistic depth maps, lower loss values are always better for the generator and a value of 0.5 to be desired for the discriminator.](assets/generator_results.png){#fig:gan_losses}

We trained the pix2pix model for around two days on a AWS p2.xlarge instance using a single core of a Tesla K80 GPU -- the results on the data was remarkable, when evaluating using a 10% test dataset. One challenge the network was still not able to solve quite successfully are gradients, as can be seen in @fig:pix2pix_nyu.


![Results from the `Pix2Pix` model on NYU trained using the `Merged` dataset](assets/pix2pix_results_merged.png){#fig:pix2pix_nyu}

![Results from the `Pix2Pix` model on Make3D trained using the `Merged` dataset](assets/pix2pix_results_merged3.png){#fig:pix2pix_make3d}


## Generator
Although we only started using the generator part of the GAN network out of an itch, it prove to be quite succesfull. This probably is due to the fact that in the loss proposed by @Isola2016, the GAN loss only contributes to 1% of the total loss -- the other 99% are the normal mean square error loss between the generators output and the target depth images.

![Results from the `Generator` model](assets/generator_results.png)
