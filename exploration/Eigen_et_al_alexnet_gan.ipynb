{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigen et al 2014\n",
    "\n",
    "Eigen, David, Christian Puhrsch, and Rob Fergus. \"Depth map prediction from a single image using a multi-scale deep network.\" Advances in neural information processing systems. 2014. [[Eigen2014](https://papers.nips.cc/paper/5539-depth-map-prediction-from-a-single-image-using-a-multi-scale-deep-network.pdf)]\n",
    "\n",
    "## Datasets: \n",
    "- [Make3D](http://make3d.cs.cornell.edu/data.html#make3d)\n",
    "- [NYU](http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.misc import imresize\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make3D data\n",
    "from Make3D import train_pairs, test_pairs\n",
    "\n",
    "# NYU data\n",
    "# from NYU import nyu_data\n",
    "# train_pairs, test_pairs = nyu_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize samples from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axis = plt.subplots(5, 2, figsize=(10,20))\n",
    "plt.tight_layout()\n",
    "for (rgb, d), (ax1, ax2) in zip(train_pairs[:10], axis):\n",
    "    ax1.axis('off'), ax2.axis('off')\n",
    "    ax1.imshow(rgb)\n",
    "    ax2.imshow(imresize(d, rgb.shape))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take the paper's convolutional network approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplify dataset first\n",
    "- Convert to grayscale\n",
    "- Scale targets down so the convolutional network can use striding and so we do not need padding\n",
    "- Normalize values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_targets = zip(*train_pairs)\n",
    "test_data, test_targets = zip(*test_pairs)\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "# Reshape data/targets to what the paper uses in order to use the same network.\n",
    "train_data = [imresize(rgb2gray(img), (304, 228))/255 for img in train_data]\n",
    "train_targets = [imresize(img, (74, 55))/255 for img in train_targets]\n",
    "test_data = [imresize(rgb2gray(img), (304, 228))/255 for img in test_data]\n",
    "test_targets = [imresize(img, (74, 55))/255 for img in test_targets]\n",
    "\n",
    "train_x, train_t = np.asarray(train_data), np.asarray(train_targets)\n",
    "test_x, test_t = np.asarray(test_data), np.asarray(test_targets)\n",
    "\n",
    "print('train input/target shapes', train_data[0].shape, train_targets[0].shape)\n",
    "print('train input min/max/ptp', np.min(train_data), np.max(train_data), np.ptp(train_data))\n",
    "print('train target min/max/ptp', np.min(train_targets), np.max(train_targets), np.ptp(train_targets))\n",
    "\n",
    "tuples = zip(train_x[:10], train_t[:10])\n",
    "fig, axis = plt.subplots(5, 2, figsize=(10,20))\n",
    "plt.tight_layout(), plt.gray()\n",
    "for (rgb, d), (ax1, ax2) in zip(tuples, axis):\n",
    "    ax1.axis('off'), ax2.axis('off')\n",
    "    ax1.imshow(rgb)\n",
    "    ax2.imshow(imresize(d, rgb.shape))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alexnet import alexnet_v2\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 304, 228))\n",
    "t = tf.placeholder(tf.float32, (None, 74, 55))\n",
    "training = tf.placeholder_with_default(False, None)\n",
    "t_ = tf.expand_dims(t, 3) \n",
    "x_ = tf.expand_dims(x, 3)  # conv2d expects a channel dimension\n",
    "\n",
    "def generator(x):\n",
    "    # coarse network implementation\n",
    "    coarse = tf.layers.conv2d(x, filters=96, kernel_size=11, strides=4, activation=tf.nn.relu)\n",
    "    coarse = tf.layers.max_pooling2d(coarse, pool_size=2, strides=2)\n",
    "    coarse = tf.layers.conv2d(coarse, filters=256, kernel_size=5, activation=tf.nn.relu, padding='same')\n",
    "    coarse = tf.layers.max_pooling2d(coarse, pool_size=2, strides=2)\n",
    "    coarse = tf.layers.conv2d(coarse, filters=384, kernel_size=3, activation=tf.nn.relu, padding='same')\n",
    "    coarse = tf.layers.conv2d(coarse, filters=384, kernel_size=3, activation=tf.nn.relu, padding='same')\n",
    "    coarse = tf.layers.conv2d(coarse, filters=256, kernel_size=3, activation=tf.nn.relu, strides=2)\n",
    "    coarse = tf.reshape(coarse, (-1, 8*6*256))\n",
    "    coarse = tf.layers.dense(coarse, units=4096, activation=tf.nn.relu)\n",
    "    coarse = tf.layers.dropout(coarse, rate=.5, training=training)  # kill neurons in training\n",
    "    coarse = tf.layers.dense(coarse, units=(74*55))\n",
    "    coarse = tf.reshape(coarse, (-1, 74, 55, 1))\n",
    "\n",
    "    # fine network implementation\n",
    "    fine = tf.layers.conv2d(x_, filters=63, kernel_size=9, strides=2, activation=tf.nn.relu)\n",
    "    fine = tf.layers.max_pooling2d(fine, pool_size=2, strides=2)\n",
    "    fine = tf.concat([fine, coarse], 3)  # join with coarse output\n",
    "    fine = tf.layers.conv2d(fine, filters=64, kernel_size=5, activation=tf.nn.relu, padding='same')\n",
    "    fine = tf.layers.conv2d(fine, filters=1, kernel_size=5, padding='same')\n",
    "\n",
    "    #y = tf.squeeze(fine, axis=3)  # remove channel dimension\n",
    "    return fine #y\n",
    "\n",
    "def discriminator(x):\n",
    "    resized = tf.image.resize_images(x, (224,224))\n",
    "    D_logit, _ = alexnet_v2(resized,\n",
    "               num_classes=2, \n",
    "               is_training=training,\n",
    "               dropout_keep_prob=0.5,\n",
    "               spatial_squeeze=True)\n",
    "    D_prob = tf.nn.sigmoid(D_logit)  # scalars between 0 and 1, 1 being its a real depthmap\n",
    "    return D_prob, D_logit\n",
    "    \n",
    "G_sample = generator(x_)\n",
    "discriminator_template = tf.make_template('discriminator', discriminator)\n",
    "D_real, D_logit_real = discriminator_template(t_)\n",
    "D_fake, D_logit_fake = discriminator_template(G_sample)\n",
    "\n",
    "# dl2 = tf.log(1. - D_fake)\n",
    "# dl1 = tf.log(D_real)\n",
    "# # dl2 = tf.Print(dl2, [dl2, dl1])\n",
    "# D_loss_real = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=D_logit_real, labels=tf.one_hot(tf.zeros_like(D_logit_real), 2))\n",
    "# D_loss_fake = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.one_hot(tf.ones_like(D_logit_fake), 2))\n",
    "# D_loss = tf.reduce_mean(D_loss_real + D_loss_fake)\n",
    "# G_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.one_hot(tf.ones_like(D_logit_fake), 2))\n",
    "\n",
    "# D_loss = -tf.reduce_mean(dl1 + dl2)\n",
    "# G_loss = tf.reduce_mean(1 - tf.log(D_fake))\n",
    "\n",
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "D_loss = D_loss_real + D_loss_fake\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))\n",
    "\n",
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss)#, var_list=theta_D)\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss)\n",
    "\n",
    "    \n",
    "#loss = tf.reduce_mean(tf.square(t - y))  # TODO: Implement loss from paper\n",
    "\n",
    "#nr_pixel = 74*55*32.\n",
    "#nr_missing = tf.reduce_sum(tf.cast(t==0, tf.float32))\n",
    "\n",
    "#nr_missing = tf.Print(nr_missing, [nr_missing])\n",
    "\n",
    "# n = tf.count_nonzero(y)\n",
    "# d = y - tf.log(t)\n",
    "# dsq = tf.square(d)\n",
    "\n",
    "# loss = tf.reduce_mean( (1/n) * tf.reduce_sum(dsq, name=\"sum1\") - (1/n**2) * (tf.reduce_sum(d))**2 )\n",
    "\n",
    "# optimizer = tf.train.AdamOptimizer(0.0001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Regularily evaluate the loss on the test data and compute test predictions when done with training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import stdout\n",
    "\n",
    "def print_progress(iteration, total, prefix='', suffix='', length=50):\n",
    "    percent = '{0:.2f}'.format(100 * (iteration / float(total)))\n",
    "    filledLength = int(round(length * iteration / float(total)))\n",
    "    bar = '=' * filledLength + '-' * (length - filledLength)\n",
    "    stdout.write('\\r%s |%s| %s%s %s' % (prefix, bar, percent, '%', suffix))\n",
    "    if iteration == total:\n",
    "        stdout.write('\\n')\n",
    "    stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "\n",
    "gloss = 1\n",
    "dloss = 0.99\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(1, EPOCHS + 1):\n",
    "    batch = np.random.permutation(len(train_t))[:32]\n",
    "    batch_x, batch_t = train_x[batch], train_t[batch]\n",
    "    if gloss > dloss:\n",
    "        sess.run(G_solver, {x: batch_x, training: True})\n",
    "    else: \n",
    "        sess.run(D_solver, {x: batch_x, t: batch_t, training: True})\n",
    "    dloss, gloss = sess.run([D_loss, G_loss], \n",
    "                            {x: test_x, t: test_t, training: False})\n",
    "    if i % 1 == 0:\n",
    "        text = 'Losses: {:.5f}, {:.5f}'.format(gloss, dloss)\n",
    "        print_progress(i, EPOCHS, suffix=text, prefix=i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_p = sess.run(G_sample, {x: test_x})\n",
    "print(sess.run([G_loss, D_loss], {x: test_x, t: test_t}))\n",
    "test_p = np.squeeze(test_p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do\n",
    "\n",
    "- loss function\n",
    "- automatic early stoppage (alle 10 epochs auf test set evaluieren -> achtung overfitting)\n",
    "- pretraining\n",
    "- local fine scale network\n",
    "- KITTI dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples = zip(test_x[:20], test_t[:20], test_p[:20])\n",
    "_, axis = plt.subplots(5, 3, figsize=(10,20))\n",
    "plt.tight_layout(), plt.gray()\n",
    "for (rgb, d, p), (ax1, ax2, ax3) in zip(triples, axis):\n",
    "    ax1.axis('off'), ax2.axis('off'), ax3.axis('off')\n",
    "    ax1.imshow(rgb)\n",
    "    ax2.imshow(imresize(d, rgb.shape))\n",
    "    ax3.imshow(imresize(p, rgb.shape))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
